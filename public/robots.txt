# Robots.txt for Gerges Samuel Portfolio
# This file tells web crawlers which pages they can and cannot access

# Allow all search engines to crawl the entire site
User-agent: *
Allow: /

# Disallow crawling of specific file types that aren't useful for SEO
Disallow: *.pdf$
Disallow: /assets/fonts/
Disallow: /node_modules/
Disallow: /_next/static/
Disallow: /.git/
Disallow: /.env*
Disallow: /package*.json
Disallow: /tsconfig*.json
Disallow: /webpack*.js
Disallow: /vite*.js

# Disallow common development and build files
Disallow: /src/
Disallow: /public/manifest.json
Disallow: /build/
Disallow: /dist/

# Allow important portfolio files
Allow: /my-main-img.webp
Allow: /logo.webp
Allow: /gerges-samuel-cv.pdf

# Allow access to portfolio images
Allow: /web/
Allow: /design/
Allow: /blog/

# Crawl delay (optional - helps prevent server overload)
Crawl-delay: 1

# Sitemap location (add this when you create a sitemap)
# Sitemap: https://yourdomain.com/sitemap.xml

# Special instructions for specific bots

# Allow Google to crawl everything
User-agent: Googlebot
Allow: /

# Allow Bing to crawl everything
User-agent: bingbot
Allow: /

# Allow other major search engines
User-agent: Slurp
Allow: /

User-agent: DuckDuckBot
Allow: /

# Disallow problematic or unwanted bots (optional)
User-agent: SemrushBot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

# Notes:
# - This robots.txt allows full access to your portfolio content
# - CV/resume PDF is allowed for download but not indexed
# - Development files are blocked to keep search results clean
# - Consider adding a sitemap.xml file for better SEO
# - Update the sitemap URL when you deploy to your actual domain